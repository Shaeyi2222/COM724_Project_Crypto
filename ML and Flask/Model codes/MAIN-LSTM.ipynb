{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9e0e3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-15 12:24:03.289149: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# First we will import the necessary Library \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For Evalution we will use these library\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, r2_score \n",
    "from sklearn.metrics import mean_poisson_deviance, mean_gamma_deviance, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# For model building we will use these library\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "# For PLotting we will use these library\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "#Important library imports...\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from numpy import set_printoptions\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2ab2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06501acd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-09-13 00:00:00</td>\n",
       "      <td>2.582778</td>\n",
       "      <td>2.590281</td>\n",
       "      <td>2.532714</td>\n",
       "      <td>2.544137</td>\n",
       "      <td>2.544137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ADA-USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-09-13 01:00:00</td>\n",
       "      <td>2.539777</td>\n",
       "      <td>2.548092</td>\n",
       "      <td>2.453710</td>\n",
       "      <td>2.463201</td>\n",
       "      <td>2.463201</td>\n",
       "      <td>47633408.0</td>\n",
       "      <td>ADA-USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-09-13 02:00:00</td>\n",
       "      <td>2.466083</td>\n",
       "      <td>2.480330</td>\n",
       "      <td>2.426486</td>\n",
       "      <td>2.430353</td>\n",
       "      <td>2.430353</td>\n",
       "      <td>117902848.0</td>\n",
       "      <td>ADA-USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-09-13 03:00:00</td>\n",
       "      <td>2.436902</td>\n",
       "      <td>2.456942</td>\n",
       "      <td>2.417653</td>\n",
       "      <td>2.417653</td>\n",
       "      <td>2.417653</td>\n",
       "      <td>70662656.0</td>\n",
       "      <td>ADA-USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-09-13 04:00:00</td>\n",
       "      <td>2.411385</td>\n",
       "      <td>2.439255</td>\n",
       "      <td>2.411385</td>\n",
       "      <td>2.427379</td>\n",
       "      <td>2.427379</td>\n",
       "      <td>109957120.0</td>\n",
       "      <td>ADA-USD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              DateTime      Open      High       Low     Close  Adj Close  \\\n",
       "0  2021-09-13 00:00:00  2.582778  2.590281  2.532714  2.544137   2.544137   \n",
       "1  2021-09-13 01:00:00  2.539777  2.548092  2.453710  2.463201   2.463201   \n",
       "2  2021-09-13 02:00:00  2.466083  2.480330  2.426486  2.430353   2.430353   \n",
       "3  2021-09-13 03:00:00  2.436902  2.456942  2.417653  2.417653   2.417653   \n",
       "4  2021-09-13 04:00:00  2.411385  2.439255  2.411385  2.427379   2.427379   \n",
       "\n",
       "        Volume   Symbol  \n",
       "0          0.0  ADA-USD  \n",
       "1   47633408.0  ADA-USD  \n",
       "2  117902848.0  ADA-USD  \n",
       "3   70662656.0  ADA-USD  \n",
       "4  109957120.0  ADA-USD  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data read in...\n",
    "crypto_data = pd.read_csv('./complete_dataset.csv')\n",
    "crypto_data.columns.values[0] = 'index'\n",
    "crypto_data.columns.values[1] = 'DateTime'\n",
    "#crypto_data.drop('index')\n",
    "crypto_data.drop(columns=['index'], inplace=True)\n",
    "crypto_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8d3add1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-09-13 00:00:00</td>\n",
       "      <td>2.582778</td>\n",
       "      <td>2.590281</td>\n",
       "      <td>2.532714</td>\n",
       "      <td>2.544137</td>\n",
       "      <td>2.544137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ADA-USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-09-13 01:00:00</td>\n",
       "      <td>2.539777</td>\n",
       "      <td>2.548092</td>\n",
       "      <td>2.453710</td>\n",
       "      <td>2.463201</td>\n",
       "      <td>2.463201</td>\n",
       "      <td>47633408.0</td>\n",
       "      <td>ADA-USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-09-13 02:00:00</td>\n",
       "      <td>2.466083</td>\n",
       "      <td>2.480330</td>\n",
       "      <td>2.426486</td>\n",
       "      <td>2.430353</td>\n",
       "      <td>2.430353</td>\n",
       "      <td>117902848.0</td>\n",
       "      <td>ADA-USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-09-13 03:00:00</td>\n",
       "      <td>2.436902</td>\n",
       "      <td>2.456942</td>\n",
       "      <td>2.417653</td>\n",
       "      <td>2.417653</td>\n",
       "      <td>2.417653</td>\n",
       "      <td>70662656.0</td>\n",
       "      <td>ADA-USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-09-13 04:00:00</td>\n",
       "      <td>2.411385</td>\n",
       "      <td>2.439255</td>\n",
       "      <td>2.411385</td>\n",
       "      <td>2.427379</td>\n",
       "      <td>2.427379</td>\n",
       "      <td>109957120.0</td>\n",
       "      <td>ADA-USD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              DateTime      Open      High       Low     Close  Adj Close  \\\n",
       "0  2021-09-13 00:00:00  2.582778  2.590281  2.532714  2.544137   2.544137   \n",
       "1  2021-09-13 01:00:00  2.539777  2.548092  2.453710  2.463201   2.463201   \n",
       "2  2021-09-13 02:00:00  2.466083  2.480330  2.426486  2.430353   2.430353   \n",
       "3  2021-09-13 03:00:00  2.436902  2.456942  2.417653  2.417653   2.417653   \n",
       "4  2021-09-13 04:00:00  2.411385  2.439255  2.411385  2.427379   2.427379   \n",
       "\n",
       "        Volume   Symbol  \n",
       "0          0.0  ADA-USD  \n",
       "1   47633408.0  ADA-USD  \n",
       "2  117902848.0  ADA-USD  \n",
       "3   70662656.0  ADA-USD  \n",
       "4  109957120.0  ADA-USD  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ensure the dataset is properly arranged by cryptocurrency and datetime stamp.\n",
    "df = crypto_data.sort_values(by=['Symbol', 'DateTime'])\n",
    "pd.to_datetime(df['DateTime'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63056db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>332775</th>\n",
       "      <td>2023-08-13 19:00:00</td>\n",
       "      <td>0.634027</td>\n",
       "      <td>0.637041</td>\n",
       "      <td>0.633796</td>\n",
       "      <td>0.635479</td>\n",
       "      <td>0.635479</td>\n",
       "      <td>10604800.0</td>\n",
       "      <td>XRP-USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332776</th>\n",
       "      <td>2023-08-13 20:00:00</td>\n",
       "      <td>0.635493</td>\n",
       "      <td>0.635806</td>\n",
       "      <td>0.631865</td>\n",
       "      <td>0.632205</td>\n",
       "      <td>0.632205</td>\n",
       "      <td>16264064.0</td>\n",
       "      <td>XRP-USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332777</th>\n",
       "      <td>2023-08-13 21:00:00</td>\n",
       "      <td>0.632201</td>\n",
       "      <td>0.632474</td>\n",
       "      <td>0.629548</td>\n",
       "      <td>0.630424</td>\n",
       "      <td>0.630424</td>\n",
       "      <td>7720768.0</td>\n",
       "      <td>XRP-USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332778</th>\n",
       "      <td>2023-08-13 22:00:00</td>\n",
       "      <td>0.630403</td>\n",
       "      <td>0.630403</td>\n",
       "      <td>0.624903</td>\n",
       "      <td>0.625863</td>\n",
       "      <td>0.625863</td>\n",
       "      <td>42362944.0</td>\n",
       "      <td>XRP-USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332779</th>\n",
       "      <td>2023-08-13 23:00:00</td>\n",
       "      <td>0.625870</td>\n",
       "      <td>0.627255</td>\n",
       "      <td>0.625636</td>\n",
       "      <td>0.626121</td>\n",
       "      <td>0.626121</td>\n",
       "      <td>10858560.0</td>\n",
       "      <td>XRP-USD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   DateTime      Open      High       Low     Close  \\\n",
       "332775  2023-08-13 19:00:00  0.634027  0.637041  0.633796  0.635479   \n",
       "332776  2023-08-13 20:00:00  0.635493  0.635806  0.631865  0.632205   \n",
       "332777  2023-08-13 21:00:00  0.632201  0.632474  0.629548  0.630424   \n",
       "332778  2023-08-13 22:00:00  0.630403  0.630403  0.624903  0.625863   \n",
       "332779  2023-08-13 23:00:00  0.625870  0.627255  0.625636  0.626121   \n",
       "\n",
       "        Adj Close      Volume   Symbol  \n",
       "332775   0.635479  10604800.0  XRP-USD  \n",
       "332776   0.632205  16264064.0  XRP-USD  \n",
       "332777   0.630424   7720768.0  XRP-USD  \n",
       "332778   0.625863  42362944.0  XRP-USD  \n",
       "332779   0.626121  10858560.0  XRP-USD  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#veiwing other portions of the data and the other charcteristics\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ff9de0",
   "metadata": {},
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf8d984d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              DateTime     Close   Symbol\n",
      "0  2021-09-13 00:00:00  2.544137  ADA-USD\n",
      "1  2021-09-13 01:00:00  2.463201  ADA-USD\n",
      "2  2021-09-13 02:00:00  2.430353  ADA-USD\n",
      "3  2021-09-13 03:00:00  2.417653  ADA-USD\n",
      "4  2021-09-13 04:00:00  2.427379  ADA-USD\n"
     ]
    }
   ],
   "source": [
    "closedf = df[['DateTime', 'Close', 'Symbol']]\n",
    "print(closedf.head())\n",
    "\n",
    "groups = closedf.groupby('Symbol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3446133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for symbol, group_df in groups:\n",
    "#     fig = px.line(group_df, x=group_df.DateTime, y=group_df.Close,labels={'DateTime':'DateTime','Close':'Close Stock'}, title=f'{symbol} - Time vs Close')\n",
    "#     fig.update_traces(marker_line_width=2, opacity=0.8, marker_line_color='orange')\n",
    "#     # fig.update_layout(title_text='Whole period of timeframe of Bitcoin close price 2014-2022', plot_bgcolor='white', \n",
    "#     #                 font_size=15, font_color='black')\n",
    "#     fig.update_xaxes(showgrid=False, type='category')\n",
    "#     fig.update_yaxes(showgrid=False)\n",
    "#     #fig.show()\n",
    "\n",
    "#     #print(group_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4666720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 \n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + time_step, 0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1130ca83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2c07e64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data:  (9983, 1)\n",
      "test_data:  (6656, 1)\n",
      "X_train:  (9967, 15)\n",
      "y_train:  (9967,)\n",
      "X_test:  (6640, 15)\n",
      "y_test (6640,)\n",
      "Epoch 1/200\n",
      "312/312 [==============================] - 5s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/200\n",
      "312/312 [==============================] - 6s 19ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/200\n",
      "312/312 [==============================] - 4s 13ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/200\n",
      "312/312 [==============================] - 5s 15ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/200\n",
      "312/312 [==============================] - 6s 19ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/200\n",
      "312/312 [==============================] - 6s 19ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/200\n",
      "312/312 [==============================] - 7s 22ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/200\n",
      "312/312 [==============================] - 6s 20ms/step - loss: nan - val_loss: nan\n",
      "Epoch 11/200\n",
      "312/312 [==============================] - 6s 20ms/step - loss: nan - val_loss: nan\n",
      "Epoch 12/200\n",
      "312/312 [==============================] - 6s 18ms/step - loss: nan - val_loss: nan\n",
      "Epoch 13/200\n",
      "312/312 [==============================] - 6s 20ms/step - loss: nan - val_loss: nan\n",
      "Epoch 14/200\n",
      "312/312 [==============================] - 4s 14ms/step - loss: nan - val_loss: nan\n",
      "Epoch 15/200\n",
      "312/312 [==============================] - 5s 15ms/step - loss: nan - val_loss: nan\n",
      "Epoch 16/200\n",
      "312/312 [==============================] - 5s 15ms/step - loss: nan - val_loss: nan\n",
      "Epoch 17/200\n",
      "312/312 [==============================] - 4s 13ms/step - loss: nan - val_loss: nan\n",
      "Epoch 18/200\n",
      "312/312 [==============================] - 7s 23ms/step - loss: nan - val_loss: nan\n",
      "Epoch 19/200\n",
      "312/312 [==============================] - 5s 17ms/step - loss: nan - val_loss: nan\n",
      "Epoch 20/200\n",
      "312/312 [==============================] - 4s 13ms/step - loss: nan - val_loss: nan\n",
      "Epoch 21/200\n",
      "312/312 [==============================] - 5s 15ms/step - loss: nan - val_loss: nan\n",
      "Epoch 22/200\n",
      "312/312 [==============================] - 5s 16ms/step - loss: nan - val_loss: nan\n",
      "Epoch 23/200\n",
      "312/312 [==============================] - 4s 12ms/step - loss: nan - val_loss: nan\n",
      "Epoch 24/200\n",
      "312/312 [==============================] - 4s 13ms/step - loss: nan - val_loss: nan\n",
      "Epoch 25/200\n",
      "312/312 [==============================] - 4s 12ms/step - loss: nan - val_loss: nan\n",
      "Epoch 26/200\n",
      "312/312 [==============================] - 5s 15ms/step - loss: nan - val_loss: nan\n",
      "Epoch 27/200\n",
      "312/312 [==============================] - 5s 15ms/step - loss: nan - val_loss: nan\n",
      "Epoch 28/200\n",
      "312/312 [==============================] - 4s 12ms/step - loss: nan - val_loss: nan\n",
      "Epoch 29/200\n",
      "312/312 [==============================] - 4s 14ms/step - loss: nan - val_loss: nan\n",
      "Epoch 30/200\n",
      "312/312 [==============================] - 4s 12ms/step - loss: nan - val_loss: nan\n",
      "Epoch 31/200\n",
      "312/312 [==============================] - 4s 12ms/step - loss: nan - val_loss: nan\n",
      "Epoch 32/200\n",
      "312/312 [==============================] - 4s 13ms/step - loss: nan - val_loss: nan\n",
      "Epoch 33/200\n",
      "312/312 [==============================] - 4s 13ms/step - loss: nan - val_loss: nan\n",
      "Epoch 34/200\n",
      "312/312 [==============================] - 4s 13ms/step - loss: nan - val_loss: nan\n",
      "Epoch 35/200\n",
      "312/312 [==============================] - 4s 13ms/step - loss: nan - val_loss: nan\n",
      "Epoch 36/200\n",
      "312/312 [==============================] - 4s 14ms/step - loss: nan - val_loss: nan\n",
      "Epoch 37/200\n",
      "312/312 [==============================] - 4s 12ms/step - loss: nan - val_loss: nan\n",
      "Epoch 38/200\n",
      "312/312 [==============================] - 4s 13ms/step - loss: nan - val_loss: nan\n",
      "Epoch 39/200\n",
      "312/312 [==============================] - 4s 12ms/step - loss: nan - val_loss: nan\n",
      "Epoch 40/200\n",
      "312/312 [==============================] - 4s 14ms/step - loss: nan - val_loss: nan\n",
      "Epoch 41/200\n",
      "312/312 [==============================] - 4s 13ms/step - loss: nan - val_loss: nan\n",
      "Epoch 42/200\n",
      "312/312 [==============================] - 4s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 43/200\n",
      "312/312 [==============================] - 6s 20ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/200\n",
      "312/312 [==============================] - 4s 12ms/step - loss: nan - val_loss: nan\n",
      "Epoch 45/200\n",
      "312/312 [==============================] - 4s 12ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/200\n",
      "312/312 [==============================] - 4s 12ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 51/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 52/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 53/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 54/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 55/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 56/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 57/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 58/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 59/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 60/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 61/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 62/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 63/200\n",
      "312/312 [==============================] - 4s 12ms/step - loss: nan - val_loss: nan\n",
      "Epoch 64/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 65/200\n",
      "312/312 [==============================] - 4s 12ms/step - loss: nan - val_loss: nan\n",
      "Epoch 66/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 67/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 68/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 69/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 70/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 71/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 72/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 73/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 74/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 75/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 76/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 77/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 78/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 79/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 80/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 81/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 82/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 83/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 84/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 85/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 86/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 87/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 88/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 89/200\n",
      "312/312 [==============================] - 4s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 90/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 91/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 92/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 93/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 94/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 95/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 96/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 97/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 98/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 99/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 100/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 101/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 102/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 103/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 104/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 105/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 106/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 107/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 108/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 109/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 110/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 111/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 112/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 113/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 114/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 115/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 116/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 117/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 118/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 119/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 120/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 121/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 122/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 123/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 124/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 125/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 126/200\n",
      "312/312 [==============================] - 4s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 127/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 128/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 129/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 130/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 131/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 132/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 133/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 134/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 135/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 136/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 137/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 138/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 139/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 140/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 141/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 142/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 143/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 144/200\n",
      "312/312 [==============================] - 4s 12ms/step - loss: nan - val_loss: nan\n",
      "Epoch 145/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 146/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 147/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 148/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 149/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 150/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 151/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 152/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 153/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 154/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 155/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 156/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 157/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 158/200\n",
      "312/312 [==============================] - 4s 12ms/step - loss: nan - val_loss: nan\n",
      "Epoch 159/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 160/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 161/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 162/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 163/200\n",
      "312/312 [==============================] - 3s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 164/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 165/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 166/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 167/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 168/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 169/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 170/200\n",
      "312/312 [==============================] - 3s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 171/200\n",
      "312/312 [==============================] - 4s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 172/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 173/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 174/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 175/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 176/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 177/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 178/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 179/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 180/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 181/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 182/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 183/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 184/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 185/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 186/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 187/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 188/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 189/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 190/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 191/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 192/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 193/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 194/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 195/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 196/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 197/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 198/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 199/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 200/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 1s 3ms/step\n",
      "208/208 [==============================] - 1s 4ms/step\n",
      "Input contains NaN.\n",
      "train_data:  (9983, 1)\n",
      "test_data:  (6656, 1)\n",
      "X_train:  (9967, 15)\n",
      "y_train:  (9967,)\n",
      "X_test:  (6640, 15)\n",
      "y_test (6640,)\n",
      "Epoch 1/200\n",
      "312/312 [==============================] - 5s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 11/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 12/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 13/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 14/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 15/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 16/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 17/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 18/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 19/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 20/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 21/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 22/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 23/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 24/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 25/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 26/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 27/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 28/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 29/200\n",
      "312/312 [==============================] - 4s 13ms/step - loss: nan - val_loss: nan\n",
      "Epoch 30/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 31/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 32/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 33/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 34/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 35/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 36/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 37/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 38/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 39/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 40/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 41/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 42/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 43/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 45/200\n",
      "312/312 [==============================] - 3s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/200\n",
      "312/312 [==============================] - 3s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 51/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 52/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 53/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 54/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 55/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 56/200\n",
      "312/312 [==============================] - 3s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 57/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 58/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 59/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 60/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 61/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 62/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 63/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 64/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 65/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 66/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 67/200\n",
      "312/312 [==============================] - 4s 13ms/step - loss: nan - val_loss: nan\n",
      "Epoch 68/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 69/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 70/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 71/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 72/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 73/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 74/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 75/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 76/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 77/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 78/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 79/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 80/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 81/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 82/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 83/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 84/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 85/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 86/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 87/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 88/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 89/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 90/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 91/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 92/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 93/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 94/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 95/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 96/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 97/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 98/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 99/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 100/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 101/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 102/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 103/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 104/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 105/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 106/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 107/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 108/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 109/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 110/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 111/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 112/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 113/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 114/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 115/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 116/200\n",
      "312/312 [==============================] - 4s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 117/200\n",
      "312/312 [==============================] - 4s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 118/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 119/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 120/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 121/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 122/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 123/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 124/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 125/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 126/200\n",
      "312/312 [==============================] - 3s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 127/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 128/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 129/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 130/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 131/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 132/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 133/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 134/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 135/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 136/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 137/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 138/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 139/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 140/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 141/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 142/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 143/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 144/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 145/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 146/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 147/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 148/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 149/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 150/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 151/200\n",
      "312/312 [==============================] - 3s 8ms/step - loss: nan - val_loss: nan\n",
      "Epoch 152/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 153/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 154/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 155/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 156/200\n",
      "312/312 [==============================] - 4s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 157/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 158/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 159/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 160/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 161/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 162/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 163/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 164/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 165/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 166/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 167/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 168/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 169/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 170/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 171/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 172/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 173/200\n",
      "312/312 [==============================] - 3s 9ms/step - loss: nan - val_loss: nan\n",
      "Epoch 174/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 175/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 176/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 177/200\n",
      "312/312 [==============================] - 5s 14ms/step - loss: nan - val_loss: nan\n",
      "Epoch 178/200\n",
      "312/312 [==============================] - 5s 14ms/step - loss: nan - val_loss: nan\n",
      "Epoch 179/200\n",
      "312/312 [==============================] - 5s 14ms/step - loss: nan - val_loss: nan\n",
      "Epoch 180/200\n",
      "312/312 [==============================] - 5s 15ms/step - loss: nan - val_loss: nan\n",
      "Epoch 181/200\n",
      "312/312 [==============================] - 4s 12ms/step - loss: nan - val_loss: nan\n",
      "Epoch 182/200\n",
      "312/312 [==============================] - 5s 15ms/step - loss: nan - val_loss: nan\n",
      "Epoch 183/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 184/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 185/200\n",
      "312/312 [==============================] - 4s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 186/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 187/200\n",
      "312/312 [==============================] - 4s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 188/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 189/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 190/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 191/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 192/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 193/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 194/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 195/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 196/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 197/200\n",
      "312/312 [==============================] - 3s 10ms/step - loss: nan - val_loss: nan\n",
      "Epoch 198/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 199/200\n",
      "312/312 [==============================] - 3s 11ms/step - loss: nan - val_loss: nan\n",
      "Epoch 200/200\n",
      "310/312 [============================>.] - ETA: 0s - loss: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 78\u001b[0m\n\u001b[1;32m     74\u001b[0m model\u001b[39m.\u001b[39madd(Dense(\u001b[39m1\u001b[39m))\n\u001b[1;32m     76\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m\"\u001b[39m,optimizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39madam\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 78\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train,y_train,validation_data\u001b[39m=\u001b[39;49m(X_test,y_test),epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m,batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     80\u001b[0m \u001b[39m#import matplotlib.pyplot as plt\u001b[39;00m\n\u001b[1;32m     82\u001b[0m loss \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.10/site-packages/keras/engine/training.py:1729\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_eval_data_handler\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1715\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mget_data_handler(\n\u001b[1;32m   1716\u001b[0m         x\u001b[39m=\u001b[39mval_x,\n\u001b[1;32m   1717\u001b[0m         y\u001b[39m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1727\u001b[0m         steps_per_execution\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution,\n\u001b[1;32m   1728\u001b[0m     )\n\u001b[0;32m-> 1729\u001b[0m val_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(\n\u001b[1;32m   1730\u001b[0m     x\u001b[39m=\u001b[39;49mval_x,\n\u001b[1;32m   1731\u001b[0m     y\u001b[39m=\u001b[39;49mval_y,\n\u001b[1;32m   1732\u001b[0m     sample_weight\u001b[39m=\u001b[39;49mval_sample_weight,\n\u001b[1;32m   1733\u001b[0m     batch_size\u001b[39m=\u001b[39;49mvalidation_batch_size \u001b[39mor\u001b[39;49;00m batch_size,\n\u001b[1;32m   1734\u001b[0m     steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m   1735\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1736\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   1737\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   1738\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   1739\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1740\u001b[0m     _use_cached_eval_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1741\u001b[0m )\n\u001b[1;32m   1742\u001b[0m val_logs \u001b[39m=\u001b[39m {\n\u001b[1;32m   1743\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()\n\u001b[1;32m   1744\u001b[0m }\n\u001b[1;32m   1745\u001b[0m epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.10/site-packages/keras/engine/training.py:2072\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2068\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   2069\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, step_num\u001b[39m=\u001b[39mstep, _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m   2070\u001b[0m ):\n\u001b[1;32m   2071\u001b[0m     callbacks\u001b[39m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 2072\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_function(iterator)\n\u001b[1;32m   2073\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   2074\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:933\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    931\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    934\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    935\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    936\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filter1Options = [\n",
    "      'APT-USD',\n",
    "      'ARB-USD',\n",
    "      'AVAX-USD',\n",
    "      'BNB-USD',\n",
    "      'BTC-USD',\n",
    "      'BCH-USD',\n",
    "      'ADA-USD',\n",
    "      'LINK-USD',\n",
    "      'DOGE-USD',\n",
    "      'ETH-USD',\n",
    "      'ETC-USD',\n",
    "      'HBAR-USD',\n",
    "      'LTC-USD',\n",
    "      'XMR-USD',\n",
    "      'MATIC-USD',\n",
    "      'SHIB-USD',\n",
    "      'SOL-USD',\n",
    "      'TRX-USD',\n",
    "      'WBTC-USD',\n",
    "      'XRP-USD'\n",
    "    ]\n",
    "\n",
    "\n",
    "#convert symbol into dummmy integer values\n",
    "\n",
    "for symbol, group_df in groups:\n",
    "    if symbol not in filter1Options:\n",
    "        print(f'{symbol} not in list...\\n')\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            #print(group_df.head())\n",
    "            use_symbol = symbol\n",
    "            new_group_df = group_df[['DateTime', 'Close']]\n",
    "            del group_df['Symbol']\n",
    "\n",
    "            #PLOT GRAPH\n",
    "            fig = px.line(new_group_df, x=new_group_df.DateTime, y=new_group_df.Close,labels={'DateTime':'DateTime','Close':'Close Stock'})\n",
    "            fig.update_traces(marker_line_width=2, opacity=0.8, marker_line_color='orange')\n",
    "            fig.update_layout(title_text=f'Whole period of timeframe of {use_symbol} close price 2014-2022', plot_bgcolor='white', \n",
    "                            font_size=15, font_color='black')\n",
    "            fig.update_xaxes(showgrid=False)\n",
    "            fig.update_yaxes(showgrid=False)\n",
    "            #fig.show()\n",
    "\n",
    "            del new_group_df['DateTime']\n",
    "            scaler=MinMaxScaler(feature_range=(0,1))\n",
    "            new_group_df=scaler.fit_transform(np.array(new_group_df).reshape(-1,1))\n",
    "            #print(new_group_df.shape)\n",
    "\n",
    "            # we keep the training set as 60% and 40% testing set\n",
    "\n",
    "            training_size=int(len(new_group_df)*0.60)\n",
    "            test_size=len(new_group_df)-training_size\n",
    "            train_data,test_data=new_group_df[0:training_size,:],new_group_df[training_size:len(new_group_df),:1]\n",
    "            print(\"train_data: \", train_data.shape)\n",
    "            print(\"test_data: \", test_data.shape)\n",
    "\n",
    "\n",
    "            time_step = 15\n",
    "            X_train, y_train = create_dataset(train_data, time_step)\n",
    "            X_test, y_test = create_dataset(test_data, time_step)\n",
    "\n",
    "            print(\"X_train: \", X_train.shape)\n",
    "            print(\"y_train: \", y_train.shape)\n",
    "            print(\"X_test: \", X_test.shape)\n",
    "            print(\"y_test\", y_test.shape)\n",
    "\n",
    "            model=Sequential()\n",
    "\n",
    "            model.add(LSTM(10,input_shape=(None,1),activation=\"relu\"))\n",
    "\n",
    "            model.add(Dense(1))\n",
    "\n",
    "            model.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n",
    "\n",
    "            history = model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=200,batch_size=32,verbose=1)\n",
    "\n",
    "            #import matplotlib.pyplot as plt\n",
    "\n",
    "            loss = history.history['loss']\n",
    "            val_loss = history.history['val_loss']\n",
    "\n",
    "            epochs = range(len(loss))\n",
    "\n",
    "            plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "            plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "            plt.title('Training and validation loss')\n",
    "            plt.legend(loc=0)\n",
    "            plt.figure()\n",
    "\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            ### Lets Do the prediction and check performance metrics\n",
    "            train_predict=model.predict(X_train)\n",
    "            test_predict=model.predict(X_test)\n",
    "            train_predict.shape, test_predict.shape\n",
    "\n",
    "            model.save(f'./All_Models/{use_symbol}.h5')\n",
    "\n",
    "            # Transform back to original form\n",
    "\n",
    "            train_predict = scaler.inverse_transform(train_predict)\n",
    "            test_predict = scaler.inverse_transform(test_predict)\n",
    "            original_ytrain = scaler.inverse_transform(y_train.reshape(-1,1)) \n",
    "            original_ytest = scaler.inverse_transform(y_test.reshape(-1,1)) \n",
    "\n",
    "            # Evaluation metrices RMSE and MAE\n",
    "            print(\"Train data RMSE: \", math.sqrt(mean_squared_error(original_ytrain,train_predict)))\n",
    "            print(\"Train data MSE: \", mean_squared_error(original_ytrain,train_predict))\n",
    "            print(\"Train data MAE: \", mean_absolute_error(original_ytrain,train_predict))\n",
    "            print(\"-------------------------------------------------------------------------------------\")\n",
    "            print(\"Test data RMSE: \", math.sqrt(mean_squared_error(original_ytest,test_predict)))\n",
    "            print(\"Test data MSE: \", mean_squared_error(original_ytest,test_predict))\n",
    "            print(\"Test data MAE: \", mean_absolute_error(original_ytest,test_predict))\n",
    "\n",
    "\n",
    "            print(\"Train data explained variance regression score:\", \n",
    "                explained_variance_score(original_ytrain, train_predict))\n",
    "            print(\"Test data explained variance regression score:\", \n",
    "                explained_variance_score(original_ytest, test_predict))\n",
    "                \n",
    "            print(\"Train data R2 score:\", r2_score(original_ytrain, train_predict))\n",
    "            print(\"Test data R2 score:\", r2_score(original_ytest, test_predict))\n",
    "\n",
    "            print(\"Train data MGD: \", mean_gamma_deviance(original_ytrain, train_predict))\n",
    "            print(\"Test data MGD: \", mean_gamma_deviance(original_ytest, test_predict))\n",
    "            print(\"----------------------------------------------------------------------\")\n",
    "            print(\"Train data MPD: \", mean_poisson_deviance(original_ytrain, train_predict))\n",
    "            print(\"Test data MPD: \", mean_poisson_deviance(original_ytest, test_predict))\n",
    "\n",
    "\n",
    "            # shift train predictions for plotting\n",
    "\n",
    "            look_back=time_step\n",
    "            trainPredictPlot = np.empty_like(new_group_df)\n",
    "            trainPredictPlot[:, :] = np.nan\n",
    "            trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict\n",
    "            print(\"Train predicted data: \", trainPredictPlot.shape)\n",
    "\n",
    "            # shift test predictions for plotting\n",
    "            testPredictPlot = np.empty_like(new_group_df)\n",
    "            testPredictPlot[:, :] = np.nan\n",
    "            testPredictPlot[len(train_predict)+(look_back*2)+1:len(new_group_df)-1, :] = test_predict\n",
    "            print(\"Test predicted data: \", testPredictPlot.shape)\n",
    "\n",
    "            names = cycle(['Original close price','Train predicted close price','Test predicted close price'])\n",
    "\n",
    "            print(group_df.head())\n",
    "\n",
    "            plotdf = pd.DataFrame({'date': group_df['DateTime'],\n",
    "                                'original_close': group_df['Close'],\n",
    "                                'train_predicted_close': trainPredictPlot.reshape(1,-1)[0].tolist(),\n",
    "                                'test_predicted_close': testPredictPlot.reshape(1,-1)[0].tolist()})\n",
    "\n",
    "            fig = px.line(plotdf,x=plotdf['date'], y=[plotdf['original_close'],plotdf['train_predicted_close'],\n",
    "                                                    plotdf['test_predicted_close']],\n",
    "                        labels={'value':'Stock price','date': 'DateTime'})\n",
    "            fig.update_layout(title_text='Comparision between original close price vs predicted close price',\n",
    "                            plot_bgcolor='white', font_size=15, font_color='black', legend_title_text='Close Price')\n",
    "            fig.for_each_trace(lambda t:  t.update(name = next(names)))\n",
    "\n",
    "            fig.update_xaxes(showgrid=False)\n",
    "            fig.update_yaxes(showgrid=False)\n",
    "            fig.show()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5d1faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5d0320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let review some datetime characteristics\n",
    "print('Earliest date is {} and Latest Date is {}'.format(df.index.min(),df.index.max()))\n",
    "time_span = df.index.max() - df.index.min()\n",
    "print('The time span of the dataset is {}'.format(time_span))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd00fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''This represent 20 months of historic data ensuring recency of the data while avoiding the impact of COVID-19 that \n",
    "impacted the cryptocurrency market and other financial markets in a haphazard manner keeping in mind that we may never be able\n",
    "to model an even tlike a PANDEMIC into our predictive model. In other words, the data points of 2020 and early 2021 are \n",
    "considered as outliers'''\n",
    "#to limit the data to a strict 20 month period, i would limit the data from 15th Dec 2021 to 15th July 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7cfd71",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_trim = df.loc['2021-12-15':'2023-07-15']\n",
    "df_trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265d74e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trim.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4525ebf",
   "metadata": {},
   "source": [
    "## EXPLORATORY DATA  ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214e12cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lets take a look at the dataset as a whole\n",
    "np.set_printoptions(precision=3, threshold=75)\n",
    "df_trim.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e0c302",
   "metadata": {},
   "source": [
    "# Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9075fb18",
   "metadata": {},
   "source": [
    "### Univariate Analysis using the BTC subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets take a look at the BTC data.\n",
    "df_BTC = df_trim.loc[df_trim['Symbol']== 'BTC-USD']\n",
    "df_BTC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645bf25d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Resample the data to a monthly frequency and calculate the mean for each month\n",
    "df_BTC_monthly = df_BTC.resample('M').mean()\n",
    "\n",
    "plt.figure(figsize=(11,6))\n",
    "\n",
    "# Create a line plot using Seaborn\n",
    "sns.lineplot(data=df_BTC_monthly, \n",
    "             x=df_BTC_monthly.index.map(lambda x: x.strftime('%m-%y')),\n",
    "             y='Adj Close'\n",
    "          )\n",
    "plt.xticks(df_BTC_monthly.index.map(lambda x: x.strftime('%m-%y')), rotation='40')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dee4a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BTC.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a26f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BTC.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50ba587",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_BTC.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4219075",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Lets create  target values(closing value of next day and a difference or gain) in the dataset \n",
    "df_BTC_2 = df_BTC.assign(close_next = df_BTC['Open'].shift(-1),\n",
    "                     diff= df_BTC['Adj Close']-df_BTC['Open']).drop('Symbol', axis=1)\n",
    "df_BTC_2\n",
    "#This will create 2 output features >>>>>Close_next >> & >>>>>Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167a42d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in order to determine if the data has any trends, a simple plot of each column is used.\n",
    "df_BTC_2.plot(subplots= True, figsize=(10,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eb2713",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Exploring the features most predictive of the target values\n",
    "plt.figure(figsize=(10, 6)) \n",
    "df_BTC_corr = round(df_BTC_2.corr(),3)\n",
    "sns.heatmap(df_BTC_corr, annot= True, cmap = 'crest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97341b35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i,column in enumerate(df_BTC_2.columns):\n",
    "    sns.histplot(df_BTC_2[column],kde=True, bins=30)  # Create the histogram using Seaborn\n",
    "    plt.title(f'Histogram of {column} amount')  # Add a title for each histogram\n",
    "#   plt.xlabel({column})  # Add x-axis label (column name)\n",
    "    plt.ylabel('Frequency')  # Add y-axis label\n",
    "    plt.tight_layout()\n",
    "    plt.show()  # Display the histogram\n",
    "# fig,((ax0,ax1), (ax2,ax3), (ax4,ax5),(ax6,ax7)) = plt.subplots(nrows=4, ncols=2)\n",
    "\n",
    "# fig,axes = plt.subplots(nrows=4, ncols=2, figsize=(10,12))\n",
    "\n",
    "# for i, columns in enumerate(df_BTC_2.columns):\n",
    "#     row = i // 2\n",
    "#     col = i % 2\n",
    "    \n",
    "#     ax = axes[row,col]\n",
    "#     ax.hist(df_BTC_2[column], bins=50)\n",
    "#     ax.set_title(f'Histogram of {column} amount')  # Add a title for each histogram\n",
    "#     ax.set_ylabel('Frequency')  # Add y-axis label\n",
    "    \n",
    "# plt.tight_layout()\n",
    "# plt.show()  # Display the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac71439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to find a way  to generalize these characteristics to the larger data that is \n",
    "# that is, the possibility that all the cryptocurrencies in the larger dataset follow these characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b894bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to explore the data properly each cryptocurrency needs to be explored independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93a0fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperate the dataset in Training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff89f7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if (df['Symbol']==df['Symbol'].shift()).all():\n",
    "#     df['previous_close'] = df.shift(1)['Adj Close'] \n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde49286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, row in crypto_data:\n",
    "#     if crypto_data(i)['Symbol'] == crypto_data.shift(i)['Symbol']:\n",
    "#         crypto_data['Diff_Close'] = crypto_data.shift(i)['Adj Close'] - crypto_data(i)['Adj Close']\n",
    "\n",
    "# crypto_data['Diff_Close'] = crypto_data.groupby('Symbol')['Adj Close'].diff().shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df2847",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df['GainOrLoss'] = df['Adj Close'] - df['previous_close']\n",
    "# df[500:700:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9b64a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9bcf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pre-processing of the data would involve extensive extensive data mungling and feature enginnering\n",
    "# Hence data splitting would be done early to avoid spilling future data in to the past.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd65581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BTC_df = (df[df['Symbol']=='BTC-USD']).sort_values(by='DateTime', ascending=False )\n",
    "\n",
    "# plt.figure(figsize=(12,10))\n",
    "    \n",
    "# ax1= sns.lineplot(x=BTC_df.index,\n",
    "#     y='Adj Close',\n",
    "#     data=BTC_df,        \n",
    "#     linewidth= 3,\n",
    "#     sort=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5281e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sktime\n",
    "# from sktime import plot_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be3408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
